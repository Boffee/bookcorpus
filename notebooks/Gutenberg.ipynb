{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sh\n",
    "sudo apt-get install libdb++-dev\n",
    "export BERKELEYDB_DIR=/usr\n",
    "pip install gutenberg patool nltk tqdm\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import itertools\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "\n",
    "from multiprocessing import Pool\n",
    "from functools import partial\n",
    "from functools import reduce\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import patoolib\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from gutenberg.acquire import get_metadata_cache\n",
    "from gutenberg.acquire import load_etext\n",
    "from gutenberg.query import get_metadata\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "os.environ['GUTENBERG_DATA'] = \"/media/brian/ColdStore/Datasets/nlp/gutenberg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_GUTENBERG_BASE_URL = \"http://www.gutenberg.org/files/\"\n",
    "\n",
    "_HTML_FILENAME_TEMPLATE = \"{idno}-h.htm\"\n",
    "_HTML_URN_TEMPLATE = \"{idno}/{idno}-h/\" + _HTML_FILENAME_TEMPLATE\n",
    "\n",
    "_TEXT_FILENAME_TEMPLATE = \"{idno}.txt\"\n",
    "_TEXT_URN_TEMPLATE = \"{idno}/\" + _TEXT_FILENAME_TEMPLATE\n",
    "\n",
    "_TEXT_FILENAME_TEMPLATE_ALT = \"{idno}-0.txt\"\n",
    "_TEXT_URN_TEMPLATE_ALT = \"{idno}/\" + _TEXT_FILENAME_TEMPLATE_ALT\n",
    "\n",
    "_SENT_FILENAME_TEMPLATE = \"{idno}.txt\"\n",
    "\n",
    "_META_FILENAME_TEMPLATE = \"{idno}.json\"\n",
    "\n",
    "_MAX_IDNO = 58120\n",
    "\n",
    "\n",
    "DEFAULT_GUTENBERG_DATA = os.environ.get('GUTENBERG_DATA') or \"~/gutenberg_data\"\n",
    "DEFAULT_GUTENBERG_DATA_HTML = os.path.join(DEFAULT_GUTENBERG_DATA, \"html\")\n",
    "DEFAULT_GUTENBERG_DATA_TEXT = os.path.join(DEFAULT_GUTENBERG_DATA, \"text\")\n",
    "DEFAULT_GUTENBERG_DATA_SENT = os.path.join(DEFAULT_GUTENBERG_DATA, \"sent\")\n",
    "DEFAULT_GUTENBERG_DATA_META = os.path.join(DEFAULT_GUTENBERG_DATA, \"meta\")\n",
    "\n",
    "PUBLIC_RIGHTS = {'Public domain in the USA.'}\n",
    "EN_LANG = {'en'}\n",
    "\n",
    "\n",
    "def populate_gutenberg_html(idnos, dest=DEFAULT_GUTENBERG_DATA_HTML):\n",
    "    pool = Pool(100)\n",
    "    download_fn = partial(download_gutenberg_book_html, dest=dest)\n",
    "    list(tqdm(pool.imap(download_fn, list(idnos)), total=len(idnos)))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "\n",
    "def populate_gutenberg_text(idnos, dest=DEFAULT_GUTENBERG_DATA_TEXT):\n",
    "    pool = Pool(100)\n",
    "    download_fn = partial(download_gutenberg_book_text, dest=dest)\n",
    "    list(tqdm(pool.imap(download_fn, list(idnos)), total=len(idnos)))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    \n",
    "def download_gutenberg_book_html(idno, dest=DEFAULT_GUTENBERG_DATA_HTML):\n",
    "    url = get_gutenberg_book_html_url(idno)\n",
    "    download(url, dest=dest)\n",
    "\n",
    "\n",
    "def download_gutenberg_book_text(idno, dest=DEFAULT_GUTENBERG_DATA_TEXT):\n",
    "    url = get_gutenberg_book_text_url(idno)\n",
    "    if not download(url, dest=dest):\n",
    "        alt_url = get_gutenberg_book_text_url(idno, use_alt=True)\n",
    "        download(alt_url, dest=dest)\n",
    "\n",
    "\n",
    "def load_gutenberg_book_text(idno, root=DEFAULT_GUTENBERG_DATA_TEXT):\n",
    "    path = get_gutenberg_book_text_local(idno)\n",
    "    if not os.path.isfile(path):\n",
    "        path = get_gutenberg_book_text_local(idno, root=root, use_alt=True)\n",
    "    if not os.path.isfile(path):\n",
    "        return None\n",
    "    try:\n",
    "        with open(path, encoding='utf-8') as fh:\n",
    "            return ''.join(fh)\n",
    "    except UnicodeDecodeError as e:\n",
    "        logging.warning(\"Using iso-8859-1: {}\".format(path))\n",
    "        with open(path, encoding='iso-8859-1') as fh:\n",
    "            return ''.join(fh)\n",
    "\n",
    "\n",
    "def load_gutenberg_book_sents(idno, root=DEFAULT_GUTENBERG_DATA_SENT):\n",
    "    path = get_gutenberg_book_sents_local(idno, root=root)\n",
    "    if not os.path.isfile(path):\n",
    "        return None\n",
    "    with open(path, encoding='utf-8') as fh:\n",
    "        return list(fh)\n",
    "\n",
    "\n",
    "def load_gutenberg_book_meta(idno, root=DEFAULT_GUTENBERG_DATA_META):\n",
    "    path = get_gutenberg_book_meta_local(idno)\n",
    "    if not os.path.isfile(path):\n",
    "        return None\n",
    "    with open(path, encoding='utf-8') as fh:\n",
    "        return json.load(fh)    \n",
    "\n",
    "\n",
    "def get_gutenberg_book_html_url(idno):\n",
    "    return os.path.join(_GUTENBERG_BASE_URL + \n",
    "                        _HTML_URN_TEMPLATE.format(idno=idno))\n",
    "\n",
    "\n",
    "def get_gutenberg_book_text_url(idno, use_alt=False):\n",
    "    urn = _TEXT_URN_TEMPLATE_ALT if use_alt else _TEXT_URN_TEMPLATE\n",
    "    return os.path.join(_GUTENBERG_BASE_URL + urn.format(idno=idno))\n",
    "\n",
    "\n",
    "def get_gutenberg_book_html_local(idno, root=DEFAULT_GUTENBERG_DATA_HTML):\n",
    "    return os.path.join(root, _HTML_FILENAME_TEMPLATE.format(idno=idno))\n",
    "\n",
    "\n",
    "def get_gutenberg_book_text_local(idno, root=DEFAULT_GUTENBERG_DATA_TEXT, use_alt=False):\n",
    "    urn = _TEXT_FILENAME_TEMPLATE_ALT if use_alt else _TEXT_FILENAME_TEMPLATE\n",
    "    return os.path.join(root, urn.format(idno=idno))\n",
    "\n",
    "\n",
    "def get_gutenberg_book_sents_local(idno, root=DEFAULT_GUTENBERG_DATA_SENT):\n",
    "    return os.path.join(root, _SENT_FILENAME_TEMPLATE.format(idno=idno))\n",
    "\n",
    "\n",
    "def get_gutenberg_book_meta_local(idno, root=DEFAULT_GUTENBERG_DATA_META):\n",
    "    return os.path.join(root, _META_FILENAME_TEMPLATE.format(idno=idno))\n",
    "\n",
    "\n",
    "def download(url, dest='/tmp/'):\n",
    "    filename = os.path.basename(url)\n",
    "    if dest[-1] == '/' or os.path.isdir(dest):\n",
    "        if not os.path.isdir(dest):\n",
    "            os.makedirs(dest)\n",
    "        dest = os.path.join(dest, filename)\n",
    "    if os.path.isfile(dest):\n",
    "        logging.info(\"{} already exist in {}.\".format(url, dest))\n",
    "    else:\n",
    "        logging.info(\"Downloading {} to {}...\".format(url, dest))\n",
    "        resp = requests.get(url)\n",
    "        if not resp.ok:        \n",
    "            logging.info(\"{}: {}\".format(resp.reason, url))\n",
    "            return False\n",
    "        with open(dest, 'wb') as fh:\n",
    "            fh.write(resp.content)\n",
    "    return True\n",
    "\n",
    "\n",
    "def unzip(filepath):\n",
    "    outdir = os.path.dirname(filepath)\n",
    "    logging.info(\"Unzipping {} to {}\".format(filepath, outdir))\n",
    "    patoolib.extract_archive(filepath, outdir=outdir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Populate metadata and book cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metacache = get_metadata_cache()\n",
    "if not metacache.exists:\n",
    "    metacache.populate()\n",
    "\n",
    "idnos = list(range(1, _MAX_IDNO+1))\n",
    "populate_gutenberg_html(idnos)\n",
    "populate_gutenberg_text(idnos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter valid english public domain book ID numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idnos = list(range(1, _MAX_IDNO+1))\n",
    "valid_idnos = list(filter(\n",
    "    lambda idno: os.path.isfile(get_gutenberg_book_text_local(idno)) or\n",
    "                 os.path.isfile(get_gutenberg_book_text_local(idno, use_alt=True)),\n",
    "    idnos))\n",
    "valid_en_idnos = list(filter(lambda idno: get_metadata('language', idno) == EN_LANG, valid_idnos))\n",
    "valid_en_public_idnos = list(filter(lambda idno: get_metadata('rights', idno) & PUBLIC_RIGHTS, valid_en_idnos))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metadata exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subject_words(idno):\n",
    "    subjs = get_metadata('subject', idno)\n",
    "    subjs_norm = map(lambda s: re.sub('[^\\w\\s]', ' ', s.lower()), subjs)\n",
    "    subjs_words = list(map(lambda s: set(s.split()), subjs_norm)) or [{'N/A'}]\n",
    "    subj_words = reduce(set.union, subjs_words)\n",
    "    return subj_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poetry_idnos = []\n",
    "for idno in valid_en_public_idnos:\n",
    "    subj_words = get_subject_words(idno)\n",
    "    if 'poetry' in subj_words:\n",
    "        poetry_idnos.append(idno)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_counts = {}\n",
    "for idno in valid_en_public_idnos:\n",
    "    keywords = get_subject_words(idno)\n",
    "    for kw in keywords:\n",
    "        keyword_counts[kw] = keyword_counts.get(kw, 0) + 1\n",
    "        \n",
    "sorted_keyword_counts = sorted(keyword_counts.items(), key=lambda kv: kv[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_counts = {}\n",
    "for idno in valid_en_public_idnos:\n",
    "    authors = get_metadata('author', idno)\n",
    "    for author in authors:\n",
    "        author_counts[author] = author_counts.get(author, 0) + 1\n",
    "\n",
    "sorted_author_counts = sorted(author_counts.items(), key=lambda kv: kv[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top100_authors = [\"Austen, Jane\", \"Dickens, Charles\", \"Twain, Mark\", \"Doyle, Arthur Conan\", \"Wilde, Oscar\", \"Stevenson, Robert Louis\", \"Shakespeare, William\", \"Shelley, Mary Wollstonecraft\", \"Conrad, Joseph\", \"Carroll, Lewis\", \"Swift, Jonathan\", \"Ibsen, Henrik\", \"Plato\", \"Homer\", \"Poe, Edgar Allan\", \"Jowett, Benjamin\", \"Hawthorne, Nathaniel\", \"Dostoyevsky, Fyodor\", \"Melville, Herman\", \"Hall, J. Lesslie (John Lesslie)\", \"Wells, H. G. (Herbert George)\", \"Stoker, Bram\", \"Pope, Alexander\", \"Irving, Washington\", \"Doré, Gustave\", \"Tolstoy, Leo, graf\", \"Nietzsche, Friedrich Wilhelm\", \"Kafka, Franz\", \"Garnett, Constance\", \"Joyce, James\", \"Dante Alighieri\", \"Dumas, Alexandre\", \"James, Henry\", \"Grimm, Jacob\", \"Grimm, Wilhelm\", \"Buckley, Theodore Alois\", \"Wyllie, David\", \"Baum, L. Frank (Lyman Frank)\", \"Kipling, Rudyard\", \"Verne, Jules\", \"Gilman, Charlotte Perkins\", \"Robertson, James Alexander\", \"Chekhov, Anton Pavlovich\", \"Blair, Emma Helen\", \"Bourne, Edward Gaylord\", \"Montgomery, L. M. (Lucy Maud)\", \"Du Bois, W. E. B. (William Edward Burghardt)\", \"Hugo, Victor\", \"Morley, Henry\", \"Chesterton, G. K. (Gilbert Keith)\", \"Russell, Bertrand\", \"Brontë, Charlotte\", \"Franklin, Benjamin\", \"Hobbes, Thomas\", \"Goethe, Johann Wolfgang von\", \"London, Jack\", \"Shaw, Bernard\", \"Cervantes Saavedra, Miguel de\", \"Lang, Andrew\", \"Kemble, E. W. (Edward Windsor)\", \"Wharton, Edith\", \"Cary, Henry Francis\", \"Maude, Louise\", \"Bierce, Ambrose\", \"Defoe, Daniel\", \"Machiavelli, Niccolò\", \"Maude, Aylmer\", \"Leech, John\", \"Alcott, Louisa May\", \"Mill, John Stuart\", \"Townsend, F. H. (Frederick Henry)\", \"Barrie, J. M. (James Matthew)\", \"Widger, David\", \"Thoreau, Henry David\", \"Marriott, W. K. (William Kenaz)\", \"Hesse, Hermann\", \"Douglass, Frederick\", \"Marlowe, Christopher\", \"Wodehouse, P. G. (Pelham Grenville)\", \"Balzac, Honoré de\", \"Voltaire\", \"Scott, Walter\", \"Malory, Thomas, Sir\", \"Chaucer, Geoffrey\", \"Burroughs, Edgar Rice\", \"Burnett, Frances Hodgson\", \"Smith, E. Boyd (Elmer Boyd)\", \"Potter, Beatrix\", \"Burton, Richard Francis, Sir\", \"Emerson, Ralph Waldo\", \"Maupassant, Guy de\", \"Hardy, Thomas\", \"Madison, James\", \"Darwin, Charles\", \"Hamilton, Alexander\", \"Jay, John\", \"Pine, Frank Woodworth\", \"Hapgood, Isabel Florence\", \"Eliot, George\", \"Ormsby, John\"]\n",
    "\n",
    "for i, author in enumerate(top100_authors):\n",
    "    if author not in author_counts:\n",
    "        print(i, author, \"N/A\")\n",
    "    elif author_counts[author] < 7:\n",
    "        print(i, author, author_counts[author])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse book HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idno = 42\n",
    "path = get_gutenberg_book_html_local(idno)\n",
    "pprint(get_metadata('subject', idno))\n",
    "pprint(get_metadata('title', idno))\n",
    "pprint(get_metadata('language', idno))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path) as fh:\n",
    "    soup = BeautifulSoup(fh, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i, par in enumerate(soup.find_all('p')):\n",
    "    text = par.text\n",
    "    print(re.sub('\\s*\\n+\\s*', ' ', text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does not work well because iso-8859-1 are silently parsed incorrectly. Directly parse book text file instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse book text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_CR = r'\\r'\n",
    "_CF = r'\\n'\n",
    "_PARA_BREAK = r'\\n\\n+'\n",
    "_LINE_BREAK = r'\\s*\\n\\s*'\n",
    "_END_WS_DASH = r'([^-]\\s\\-{1,3})' + _LINE_BREAK\n",
    "_START_DASH_WS = _LINE_BREAK + r'(\\-{2,3}\\s[^-])'\n",
    "_END_DASH = r'([^-]\\-{1,3})' + _LINE_BREAK\n",
    "_START_DASH = _LINE_BREAK + r'(\\-{2,3}[^-])'\n",
    "\n",
    "_CR_RE = re.compile(_CR)\n",
    "_CF_RE = re.compile(_CF)\n",
    "_PARA_BREAK_RE = re.compile(_PARA_BREAK)\n",
    "_LINE_BREAK_RE = re.compile(_LINE_BREAK)\n",
    "_END_WS_DASH_RE = re.compile(_END_WS_DASH)\n",
    "_START_DASH_WS_RE = re.compile(_START_DASH_WS)\n",
    "_END_DASH_RE = re.compile(_END_DASH)\n",
    "_START_DASH_RE = re.compile(_START_DASH)\n",
    "\n",
    "\n",
    "def remove_carriage_return(text):\n",
    "    return _CR_RE.sub('', text)\n",
    "\n",
    "\n",
    "def paragraph_segment(text):\n",
    "    return _PARA_BREAK_RE.split(text)\n",
    "\n",
    "\n",
    "def is_poetry(text, threshold=0.95):\n",
    "    '''Check if beginning of every line is capitalized.'''\n",
    "    lines = _LINE_BREAK_RE.split(text)\n",
    "    total_lines = len(lines)\n",
    "    bad_lines = 0\n",
    "    \n",
    "    if len(lines) < 2:\n",
    "        return False\n",
    "    \n",
    "    for line in lines:\n",
    "        if line and line[0].islower():\n",
    "            bad_lines += 1\n",
    "\n",
    "    return bad_lines/total_lines < (1 - threshold)\n",
    "\n",
    "\n",
    "def stitch_paragraph(text):\n",
    "    # must sub dash with ws first\n",
    "    text = _END_WS_DASH_RE.sub('\\\\1 ', text)\n",
    "    text = _START_DASH_WS_RE.sub(' \\\\1', text)\n",
    "    text = _END_DASH_RE.sub('\\\\1', text)\n",
    "    text = _START_DASH_RE.sub('\\\\1', text)\n",
    "    text = _LINE_BREAK_RE.sub(' ', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def sentence_segment(text):\n",
    "    chunks = _CF_RE.split(text)\n",
    "    chunks_sents = map(sent_tokenize, chunks)\n",
    "    sents = itertools.chain.from_iterable(chunks_sents)\n",
    "    return sents\n",
    "\n",
    "\n",
    "def extract_sentences(text):\n",
    "    if not text:\n",
    "        return []\n",
    "    text_norm = remove_carriage_return(text)\n",
    "    paras = paragraph_segment(text_norm)\n",
    "    paras_norm = map(lambda p: p if is_poetry(p) else stitch_paragraph(p),\n",
    "                     paras)\n",
    "    return itertools.chain.from_iterable(map(sentence_segment, paras_norm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Poetry Detection\n",
    "Basic analysis of poetry detection shows that simple Capitalization detection is fairly consistant without any additional tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poetry_percent_dict = {}\n",
    "for idno in poetry_idnos:\n",
    "    text = load_gutenberg_book_text(idno)\n",
    "    if not text:\n",
    "        continue\n",
    "    text = remove_carriage_return(text)\n",
    "    paragraphs = paragraph_segment(text)\n",
    "    p_count = 0\n",
    "    np_count = 0\n",
    "    for par in paragraphs:\n",
    "        if is_poetry(par):\n",
    "            p_count += 1\n",
    "        else:\n",
    "            np_count += 1\n",
    "    poetry_percent_dict[idno] = p_count/(p_count+np_count)\n",
    "\n",
    "poetry_percents = list(poetry_percent_dict.values())\n",
    "poetry_percent_mean = np.mean(poetry_percents)\n",
    "poetry_percent_std = np.std(poetry_percents)\n",
    "plt.hist(poetry_percents, bins=100)\n",
    "plt.xlabel(\"poetry paragraph percentage\")\n",
    "plt.ylabel(\"number of books\")\n",
    "pprint(\"mean: {}, std: {}\".format(poetry_percent_mean, poetry_percent_std))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "human evaluation of poetry classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = load_gutenberg_book_text(poetry_idnos[42])\n",
    "text = remove_carriage_return(text)\n",
    "paragraphs = paragraph_segment(text)\n",
    "for par in paragraphs:\n",
    "    print(par)\n",
    "    if is_poetry(par):\n",
    "        print('#'*80)\n",
    "    else:\n",
    "        print('. '*40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Sentence Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idno=42\n",
    "\n",
    "text = load_gutenberg_book_text(idno)\n",
    "pprint(get_metadata('subject', idno))\n",
    "pprint(get_metadata('title', idno))\n",
    "\n",
    "sents = list(extract_sentences(text))\n",
    "\n",
    "sent_lens = [len(s) for s in sents]\n",
    "sent_len_mean = np.mean(sent_lens)\n",
    "sent_len_std = np.std(sent_lens)\n",
    "\n",
    "plt.hist(sent_lens, bins=100)\n",
    "plt.xlabel(\"sentence length (chars)\")\n",
    "plt.ylabel(\"number of sentences\")\n",
    "pprint(\"mean: {}, std: {}\".format(sent_len_mean, sent_len_std)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Books to sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _init_extract_gutenberg_sentences(lang):\n",
    "    extract_gutenberg_sentences.segmenter = load_segmenter(lang)\n",
    "\n",
    "    \n",
    "def extract_gutenberg_sentences(idno, lang='en'):\n",
    "    if (not hasattr(extract_gutenberg_sentences, 'segmenter') or \n",
    "        extract_gutenberg_sentences.segmenter.lang is not lang):\n",
    "        logging.warning(\"Creating tokenizer with lang: {}\".format(lang))\n",
    "        _init_extract_gutenberg_sentences(lang)\n",
    "    \n",
    "    text = load_gutenberg_book_text(idno)\n",
    "    return list(extract_sentences(text, extract_gutenberg_sentences.segmenter))\n",
    "\n",
    "\n",
    "def generate_gutenberg_sentences(idno, lang='en'):\n",
    "    cache_path = get_gutenberg_book_sents_local(idno)\n",
    "    if os.path.isfile(cache_path):\n",
    "        return\n",
    "    cache_dir = os.path.dirname(cache_path)\n",
    "    if not os.path.isdir(cache_dir):\n",
    "        os.makedirs(cache_dir)\n",
    "    sents = extract_gutenberg_sentences(idno, lang=lang)\n",
    "    if not sents:\n",
    "        return\n",
    "    with open(cache_path, 'w', encoding='utf-8') as fh:\n",
    "        fh.write('\\n'.join(sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pool = Pool(processes=4)\n",
    "_ = list(tqdm(pool.imap(generate_gutenberg_sentences, range(_MAX_IDNO), chunksize=10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write metadata json\n",
    "\n",
    "Store preprocessed metadata because the raw metadata can easily be extracted using `get_metadata`\n",
    "\n",
    "**preprocessing:**:\n",
    "- `author` [categorical] : as is, use each unique author as a class (will probably need to take top $n$ authors. Alternatively, lower case, remove all punctuations except period (because period is used for abbreviation), although this is probably not a good idea because sharing embedding for names is likely useless.\n",
    "- `language` [categorical] : as is.\n",
    "- `subject`[language] : lower case, replace punctuations with space, take unique space separated words.\n",
    "- `title` [categorical] : as is, although this is probably not practical becauses there are too many unique titles to store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subject_words(idno):\n",
    "    subjs = get_metadata('subject', idno)\n",
    "    subjs_norm = map(lambda s: re.sub('[^\\w\\s]', ' ', s.lower()), subjs)\n",
    "    subjs_words = list(map(lambda s: set(s.split()), subjs_norm)) or [{'N/A'}]\n",
    "    subj_words = reduce(set.union, subjs_words)\n",
    "    return subj_words\n",
    "\n",
    "def extract_gutenberg_meta(idno):\n",
    "    metadata = {}\n",
    "    metadata['author'] = list(get_metadata('author', idno))\n",
    "    metadata['language'] = list(get_metadata('language', idno))\n",
    "    metadata['title'] = list(get_metadata('title', idno))\n",
    "    metadata['subject'] = list(get_subject_words(idno))\n",
    "    return metadata\n",
    "\n",
    "def generate_gutenberg_meta(idno):\n",
    "    cache_path = get_gutenberg_book_meta_local(idno)\n",
    "    if os.path.isfile(cache_path):\n",
    "        return\n",
    "    cache_dir = os.path.dirname(cache_path)\n",
    "    if not os.path.isdir(cache_dir):\n",
    "        os.makedirs(cache_dir)\n",
    "    metadata = extract_gutenberg_meta(idno)\n",
    "    with open(cache_path, 'w', encoding='utf-8') as fh:\n",
    "        json.dump(metadata, fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = Pool(processes=4)\n",
    "_ = list(tqdm(pool.imap(generate_gutenberg_meta, range(_MAX_IDNO), chunksize=10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Text and Post-Text removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_tags = [\n",
    "    \"*** START OF THIS PROJECT GUTENBERG EBOOK\",\n",
    "    \"*** START OF THE PROJECT GUTENBERG EBOOK\",\n",
    "    \"***START OF THE PROJECT GUTENBERG EBOOK\",\n",
    "]\n",
    "    \n",
    "end_tags = [\n",
    "    \"*** END OF THIS PROJECT GUTENBERG EBOOK\",\n",
    "    \"*** END OF THE PROJECT GUTENBERG EBOOK\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idno = 137\n",
    "has_start = 0\n",
    "has_end = 0\n",
    "for sent in load_gutenberg_book_sents(idno):\n",
    "    tagged = False\n",
    "    for tag in start_tags:\n",
    "        if sent.startswith(tag):\n",
    "            has_start += 1\n",
    "            tagged = True\n",
    "            break\n",
    "    if tagged:\n",
    "        continue\n",
    "    for tag in end_tags:\n",
    "        if sent.startswith(tag):\n",
    "            has_end += 1\n",
    "            break\n",
    "\n",
    "print('idno: {} starts: {} ends: {}'.format(idno, has_start, has_end))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
